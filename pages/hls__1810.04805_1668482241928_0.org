#+file-path: ../assets/1810.04805_1668482241928_0.pdf
#+file: [[../assets/1810.04805_1668482241928_0.pdf][1810.04805_1668482241928_0.pdf]]
#+title: hls__1810.04805_1668482241928_0

* ERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. 
:PROPERTIES:
:ls-type: annotation
:hl-page: 1
:hl-color: yellow
:id: 637304d8-61d5-441d-9edb-516e4abe6631
:END:
* Language model pre-training has been shown to be effective for improving many natural language processing tasks
:PROPERTIES:
:ls-type: annotation
:hl-page: 1
:hl-color: yellow
:id: 6373061b-027f-444f-858f-0f2f7f34068d
:END:
* There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.
:PROPERTIES:
:ls-type: annotation
:hl-page: 1
:hl-color: yellow
:id: 63730634-2ec4-40a4-8007-55085a440d5f
:END:
* The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training
:PROPERTIES:
:ls-type: annotation
:hl-page: 1
:hl-color: yellow
:id: 6373064c-7eb6-4e15-b819-90b07c578316
:END:
* we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a “masked language model” (MLM) pre-training objective, inspired by the Cloze task (Taylor, 1953).
:PROPERTIES:
:ls-type: annotation
:hl-page: 1
:hl-color: yellow
:id: 6373066b-b0a8-49ab-87f1-f2b3b0eb0529
:END:
* The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked arXiv:1810.04805v2 [cs.CL] 24 May 2019 word based only on its context. Unlike left-to-
:PROPERTIES:
:ls-type: annotation
:hl-page: 1
:hl-color: yellow
:id: 63730722-afd6-42d5-95c7-99ff40338ffd
:END:
* Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch
:PROPERTIES:
:ls-type: annotation
:hl-page: 2
:hl-color: yellow
:id: 6373078b-ae03-4fff-8b24-41800b385244
:END:
* ELMo and its predecessor (Peters et al., 2017,2018a) generalize traditional word embedding research along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. 
:PROPERTIES:
:ls-type: annotation
:hl-page: 2
:hl-color: yellow
:id: 637307a3-25e6-4edb-9a46-d25711b008ad
:END:
* Melamud et al. (2016) proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional. 
:PROPERTIES:
:ls-type: annotation
:hl-page: 2
:hl-color: yellow
:id: 637307d5-5dcc-45d3-af16-27245b98f137
:END:
* More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018).
:PROPERTIES:
:ls-type: annotation
:hl-page: 2
:hl-color: yellow
:id: 637307e5-5de0-4d1a-bdba-e169c92b256c
:END:
* omputer vision research has also demonstrated the importance of transfer learning from large pre-trained models, where an effective recipe is to fine-tune models pre-trained with ImageNet (Deng et al., 2009; Yosinski et al., 2014).
:PROPERTIES:
:ls-type: annotation
:hl-page: 3
:hl-color: yellow
:id: 637307fb-866c-4205-b42a-691105dbe407
:END:
* There are two steps in our framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.
:PROPERTIES:
:ls-type: annotation
:hl-page: 3
:hl-color: yellow
:id: 63730815-3dc0-404e-8022-e1a55c903d40
:END:
* A distinctive feature of BERT is its unified architecture across different tasks
:PROPERTIES:
:ls-type: annotation
:hl-page: 3
:hl-color: yellow
:id: 63730825-f4a0-49f6-b0e9-614729e1e18c
:END:
* To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences(e.g., 〈 Question, Answer 〉) in one token sequence.
:PROPERTIES:
:ls-type: annotation
:hl-page: 4
:hl-color: yellow
:id: 63730856-9e3d-419e-ae83-8ba7a30c96b0
:END:
* In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens
:PROPERTIES:
:ls-type: annotation
:hl-page: 4
:hl-color: yellow
:id: 63730882-ab21-43be-983c-4c68038b6cc0
:END:
* In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than reconstructing the entire input.
:PROPERTIES:
:ls-type: annotation
:hl-page: 4
:hl-color: yellow
:id: 637308a9-3e4b-4252-9b5a-c8417a011918
:END:
* Although this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not appear during fine-tuning. To mitigate this, we do not always replace “masked” words with the actual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction.
:PROPERTIES:
:ls-type: annotation
:hl-page: 4
:hl-color: yellow
:id: 637308c2-519b-420b-a363-56bd4f068692
:END:
* However, in prior work, only sentence embeddings are transferred to down-stream tasks, where BERT transfers all parameters to initialize end-task model parameters.
:PROPERTIES:
:ls-type: annotation
:hl-page: 5
:hl-color: yellow
:id: 637308ec-b4c5-432b-95e1-75f34ab216da
:END:
* It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences.
:PROPERTIES:
:ls-type: annotation
:hl-page: 5
:hl-color: yellow
:id: 63730903-fcf8-4bca-bd8e-eaeea87ae95a
:END:
* Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream tasks— whether they involve single text or text pairs—by swapping out the appropriate inputs and outputs.
:PROPERTIES:
:ls-type: annotation
:hl-page: 5
:hl-color: yellow
:id: 63730912-dcfd-449a-aca6-8be10664356e
:END:
* BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.
:PROPERTIES:
:ls-type: annotation
:hl-page: 5
:hl-color: yellow
:id: 63730925-e016-4fe6-bbbc-016c46286e90
:END:
* Compared to pre-training, fine-tuning is relatively inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model
:PROPERTIES:
:ls-type: annotation
:hl-page: 5
:hl-color: yellow
:id: 63730946-0e3f-497d-8426-5cfaa6b163c2
:END:
* Additionally, for BERTLARGE we found that finetuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set.
:PROPERTIES:
:ls-type: annotation
:hl-page: 6
:hl-color: yellow
:id: 63730975-31f5-43c0-a83d-81421bea8b90
:END:
* We use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat questions that do not have an answer as having an answer span with start and end at the [CLS] token. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token.
:PROPERTIES:
:ls-type: annotation
:hl-page: 7
:hl-color: yellow
:id: 637309d2-7b27-485a-ae7d-4472f1d26e61
:END:
* For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no rightside context. In order to make a good faith attempt at strengthening the LTR system, we added a randomly initialized BiLSTM on top. 
:PROPERTIES:
:ls-type: annotation
:hl-page: 8
:hl-color: yellow
:id: 63730a15-93d7-47d5-a580-5f4d0e722915
:END:
* We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models, as ELMo does. However: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer.
:PROPERTIES:
:ls-type: annotation
:hl-page: 8
:hl-color: yellow
:id: 63730a2f-6255-4241-b148-3d19ca68ca9c
:END:
* It has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6. However, we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained. 
:PROPERTIES:
:ls-type: annotation
:hl-page: 8
:hl-color: yellow
:id: 63730a52-1f8e-4922-b810-cc29c6a733fc
:END:
* First, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added. Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation.
:PROPERTIES:
:ls-type: annotation
:hl-page: 9
:hl-color: yellow
:id: 63730a6b-badf-45ee-b7bd-120099e6298e
:END: